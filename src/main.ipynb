{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdevtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResearchSchema, WebSearchEvent, ScrapeURLEvent, ScrapeResultsSchema\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_ollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'schema'"
     ]
    }
   ],
   "source": [
    "from ddg import Duckduckgo\n",
    "from duckduckgo_search import DDGS\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "from pydantic_ai.models.ollama import OllamaModel\n",
    "import random\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from devtools import debug\n",
    "from schema.schema import ResearchSchema, WebSearchEvent, ScrapeURLEvent, ScrapeResultsSchema\n",
    "from langchain_ollama import ChatOllama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    Context,\n",
    "    step\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Archit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\market-research-agent-m8UpT5-t-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Complete\n",
      "Running step _webSearch\n",
      "Step _webSearch produced event WebSearchEvent\n",
      "Running step _scrapeURL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping search results: 100%|██████████| 2/2 [00:20<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step _scrapeURL produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "from workflows.web_search_workflow import WebSearchWorkflow\n",
    "from utils.settings import project_settings\n",
    "\n",
    "print(\"Config Complete\")\n",
    "\n",
    "search_workflow = WebSearchWorkflow(timeout=60, verbose=True)\n",
    "result = await search_workflow.run(search_query = \"Tata Motors vision\", jina_api_key = project_settings.JINA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION\n",
      "\n",
      "As a high-performance Organization, we are by FY2019\n",
      "\n",
      "*   Among the Top 3 Global CV and Domestic PV\n",
      "*   Achieving Sustainable Financial Performance\n",
      "*   Delivering Exciting Innovation\n",
      "\n",
      "To become the most aspirational  \n",
      "Indian auto brand by 2024\n",
      "\n",
      "Our vision is to be a world-class, future-ready organization, driven by a passion for innovation, customer-centricity, and sustainability. We aim to be among the top three global commercial vehicle and domestic passenger vehicle manufacturers, achieving sustainable financial performance and delivering exciting innovations. \n",
      "\n",
      "Our mission is to innovate mobility solutions with passion to enhance the quality of life. We are committed to making responsible choices, creating a positive legacy, and connecting aspirations. \n",
      "\n",
      "We have a strong focus on sustainability, with an emphasis on decarbonization, circular economy, and preservation of the natural environment. We are integrating sustainability into our business strategy and are dedicated to delivering customer satisfaction through a sustainable, no-compromise approach. \n",
      "\n",
      "Our goal is to be a leader in the automotive industry, with a strong global presence and a commitment to innovation, customer-centricity, and sustainability. We strive to create a positive impact on the environment and society, while delivering value to our customers and stakeholders.\n"
     ]
    }
   ],
   "source": [
    "print(result.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step _webSearch\n",
      "Step _webSearch produced event WebSearchEvent\n",
      "Running step _scrapeURL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping search results: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step _scrapeURL produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScrapeResultsSchema(urls=['https://tatamotors.co.id/about-us/visionmision-values/', 'https://www.tatamotors.com/organisation/about-us/'], query='Tata Motors vision', main='Tata Motors vision: \"Our vision is to be the world\\'s best automotive brand, with a focus on innovation, sustainability and customer delight. We aim to create a sustainable future for our customers, employees and the environment. Our long-term strategy is to become a leader in electric vehicles, while also expanding our portfolio of passenger and commercial vehicles.\"')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WebSearchWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def _webSearch(self, ctx: Context, ev: StartEvent)->WebSearchEvent:\n",
    "\n",
    "        search_query = ev.search_query\n",
    "\n",
    "        await ctx.set(\"search_query\", search_query)\n",
    "\n",
    "        list_ = DDGS().text(  \n",
    "                keywords = search_query,\n",
    "                region = 'wt-wt',\n",
    "                safesearch = 'off',\n",
    "                timelimit = '7d',\n",
    "                max_results = 2\n",
    "            )\n",
    "\n",
    "        return WebSearchEvent(url_list=list_)\n",
    "\n",
    "    @step\n",
    "    async def _scrapeURL(self, ctx: Context, ev: WebSearchEvent)->StopEvent:\n",
    "        url_list = ev.url_list\n",
    "        search_query:str = await ctx.get(\"search_query\")\n",
    "        \n",
    "        data = \"\"\n",
    "        source_urls = []\n",
    "        for search_result in tqdm(url_list, desc=\"Scraping search results\"):\n",
    "            fetched_raw_data = _apiRequest(url = search_result['href'])\n",
    "            data+=fetched_raw_data+\"\\n\"\n",
    "            source_urls.append(search_result['href'])\n",
    "\n",
    "\n",
    "\n",
    "        parsed_data = await _parseScrapedData(\n",
    "                scrapedData = data,\n",
    "                searchQuery = search_query\n",
    "        )\n",
    "            \n",
    "        data = ScrapeResultsSchema(urls = source_urls, query = search_query, main = parsed_data)\n",
    "        return StopEvent(result=data)\n",
    "\n",
    "search_workflow = WebSearchWorkflow(timeout=60, verbose=True)\n",
    "result = await search_workflow.run(search_query = \"Tata Motors vision\")\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_model = OllamaModel(model_name=\"llama3.2:latest\")\n",
    "chat_model = ChatOllama(model=\"llama3.2:latest\")\n",
    "# model = GroqModel('llama-3.3-70b-versatile', api_key=os.getenv('GROQ_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'pm modi book - Huge Selection Of Products - amazon.in',\n",
       "  'href': 'https://www.bing.com/aclick?ld=e8403E3nGawVVyHT7z5CMvejVUCUxDdN8EY8nXDeRFntyutzKsxqk75zn_iX9wWzzL6jHJc585Q-z-mwEgBAojUyGwh-yxMQ-jmeEBEZa3DTT0vroBS_LmqeY2iNLnvTHjNl7ms9SFuvGl5OHOQxXta9yT7dzOqDkgezDD2zTL6ZfIa2Yf9GxnfZyx-V74066SYn1a8A&u=aHR0cHMlM2ElMmYlMmZ3d3cuYW1hem9uLmluJTJmcyUyZiUzZmllJTNkVVRGOCUyNmtleXdvcmRzJTNkcG0lMmJtb2RpJTJiYm9vayUyNmluZGV4JTNkYXBzJTI2dGFnJTNkbXNuZGVza3N0ZGluLTIxJTI2cmVmJTNkcGRfc2xfdGtzdmV5ZDcxX2IlMjZhZGdycGlkJTNkMTMxMzkxODAwMDkxNzAyNSUyNmh2YWRpZCUzZDgyMTIwMTQ1OTU3NjM1JTI2aHZuZXR3JTNkcyUyNmh2cW10JTNkYiUyNmh2Ym10JTNkYmIlMjZodmRldiUzZGMlMjZodmxvY2ludCUzZCUyNmh2bG9jcGh5JTNkMTU3NjQ5JTI2aHZ0YXJnaWQlM2Rrd2QtODIxMjA3NTE2NTg4MjElM2Fsb2MtOTAlMjZoeWRhZGNyJTNkMjM2NDJfMjI5MTc5MSUyNm1jaWQlM2QlMjZtc2Nsa2lkJTNkYzZiZjVkMTQ5NDZkMThkNzc5MTUzYTY1MWZhODkxZDc&rlid=c6bf5d14946d18d779153a651fa891d7',\n",
       "  'body': 'Choose From a Wide Selection Of Informative and Comprehensive Books For You. Amazon Offers an Array Of Unique Products From Hundreds Of Brands.'},\n",
       " {'title': 'Narendra Modi - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/Narendra_Modi',\n",
       "  'body': 'Learn about the life and career of Narendra Modi, the current prime minister of India and a leader of the Bharatiya Janata Party and the Rashtriya Swayamsevak Sangh. Find out his achievements, controversies, policies, and international trips.'},\n",
       " {'title': 'Prime Minister of India',\n",
       "  'href': 'https://www.pmindia.gov.in/en/',\n",
       "  'body': \"PM's Profile. Shri Narendra Modi was sworn-in as India’s Prime Minister for the third time on 9th June 2024, following another decisive victory in the 2024 Parliamentary elections. This victory marked the third consecutive term for Shri Modi, further solidifying his leadership.\"},\n",
       " {'title': 'Narendra Modi | NarendraModi.in Official Website of Prime Minister of ...',\n",
       "  'href': 'https://www.narendramodi.in/',\n",
       "  'body': \"Narendra Modi, the 14th Prime Minister of India, is a figure whose political journey is as intriguing as inspiring. Rising from humble beginnings, Modi's ascent through the ranks of the Rashtriya Swayamsevak Sangh (…\"},\n",
       " {'title': 'Know the PM - Prime Minister of India',\n",
       "  'href': 'https://www.pmindia.gov.in/en/pms-profile/',\n",
       "  'body': 'Learn about the life and work of Narendra Modi, the Prime Minister of India for the third time since 2024. Discover his vision, initiatives and impact on various sectors such as poverty, health, education, agriculture, infrastructure and more.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ = DDGS().text(  \n",
    "                keywords = \"pm modi\",\n",
    "                region = 'wt-wt',\n",
    "                safesearch = 'off',\n",
    "                timelimit = '7d',\n",
    "                max_results = 5\n",
    "            )\n",
    "\n",
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "async def _parseScrapedData(scrapedData: str, searchQuery: str)->str:\n",
    "\n",
    "    parse_prompt = f'''\n",
    "\n",
    "    You are an advanced text extraction model. Your task is to extract only the main textual content from the provided scraped webpage data, removing all unrelated elements such as URLs, images, JavaScript, or redundant navigational text. \n",
    "    Focus on delivering clear, meaningful content that reflects the primary information and purpose of the webpage. You will also be given a search query which you will use to find the textual data, from the web scraped data, that is relevant to the search query  \n",
    "    \n",
    "    Instructions:\n",
    "     - Don't modify the text extracted from the data into your own words. Just extract the main textual data as it is.\n",
    "     - Don't say 'Here is the extracted content..' or 'This is a web page...', just remove the unwanted data from the entire input and only output the main textual data as it is.\n",
    "     - Only find out textual data that is relevant to the search query given to you\n",
    "\n",
    "    Search Query: {searchQuery}\n",
    "    Web Scraped Input: (Data to be parsed) \n",
    "\n",
    "    {scrapedData}\n",
    "\n",
    "    Output: \n",
    "\n",
    "    '''\n",
    "    return chat_model.invoke(parse_prompt).content\n",
    "\n",
    "async def _webSearch(query: str)->dict:\n",
    "    print(f\"Recieved Query: {query}. Starting Web Search\")\n",
    "    list_ = DDGS().text(  \n",
    "                keywords = query,\n",
    "                region = 'wt-wt',\n",
    "                safesearch = 'off',\n",
    "                timelimit = '7d',\n",
    "                max_results = 5\n",
    "            )\n",
    "\n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('JINA_API_KEY')}\"\n",
    "    }\n",
    "\n",
    "    data = \"\"\n",
    "\n",
    "    print(f\"Web Search Complete. Starting Scraping\")\n",
    "\n",
    "    for result in tqdm(list_, desc=\"Scraping Links\"):\n",
    "        try:\n",
    "            url = f\"https://r.jina.ai/{result['href']}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            parsed_data = await _parseScrapedData(\n",
    "                scrapedData = response.text,\n",
    "                searchQuery = query\n",
    "            )\n",
    "\n",
    "            response.raise_for_status()\n",
    "            data += parsed_data + \"\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "webSearch_agent = Agent(\n",
    "    agent_model,\n",
    "    system_prompt = '''\n",
    "        You are a web searching assistant that does web searches based on query. You must execute the  \n",
    "        You will be given a query and you must use the tool that Searches across the web using query. \n",
    "        You will not answer the query based on your data and will always rely upon web scraping results.\n",
    "    '''\n",
    ")\n",
    "\n",
    "@webSearch_agent.tool\n",
    "async def searchWeb(ctx: RunContext[str], query: str):\n",
    "    \"\"\"Search across the web using query.\n",
    "\n",
    "    Args: \n",
    "\n",
    "        query: query to search\n",
    "    \"\"\"\n",
    "    \n",
    "    list_ = DDGS().text(  \n",
    "                keywords = query,\n",
    "                region = 'wt-wt',\n",
    "                safesearch = 'off',\n",
    "                timelimit = '7d',\n",
    "                max_results = 2\n",
    "            )\n",
    "\n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('JINA_API_KEY')}\"\n",
    "    }\n",
    "\n",
    "    data = \"\"\n",
    "\n",
    "    for result in tqdm(list_, desc=f\"Scraping and Parsing Links for query {query}\"):\n",
    "        try:\n",
    "            url = f\"https://r.jina.ai/{result['href']}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            parsed_data = await _parseScrapedData(\n",
    "                scrapedData = response.text,\n",
    "                searchQuery = query\n",
    "            )\n",
    "\n",
    "            response.raise_for_status()\n",
    "            data += parsed_data + \"\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping and Parsing Links for query Tata Steel Vision: 100%|██████████| 2/2 [00:19<00:00,  9.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the web search results, it appears that Tata Steel is a steel company based in India with a long history of innovation and a strong commitment to employee welfare and safety. The company operates globally and has developed a range of business models and strategies to ensure sustainability and growth. Tata Steel offers a range of steel products and solutions for different industries and is committed to engaging with local communities and supporting development initiatives.\n"
     ]
    }
   ],
   "source": [
    "results = await webSearch_agent.run(\n",
    "    'Tata Steel Vision'\n",
    ")\n",
    "\n",
    "print(results.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeps(BaseModel):\n",
    "    response: str\n",
    "    query: str\n",
    "\n",
    "class JudgeResponseType(BaseModel):\n",
    "    similar: bool\n",
    "\n",
    "judging_agent = Agent(\n",
    "    agent_model,\n",
    "    # result_type = JudgeResponseType,\n",
    "    deps_type = MyDeps,\n",
    "    system_prompt='''\n",
    "        You are an expert agent tasked with determining if a given response answers a specific query.\n",
    "        You will be provided with:\n",
    "        - A query: a question or request for information.\n",
    "        - A response: the potential answer to the query.\n",
    "\n",
    "        Your job is to:\n",
    "        1. Fully understand the query.\n",
    "        2. Analyze whether the response directly, indirectly, or partially answers the query.\n",
    "        3. Return `true` if the response answers the query in any capacity, otherwise return `false`.\n",
    "\n",
    "        Be objective and ensure that your judgments are clear and based only on the content of the query and response.\n",
    "    '''\n",
    ")\n",
    "\n",
    "@judging_agent.tool\n",
    "async def judge_responses(ctx: RunContext[str], query: str):\n",
    "    response =  ctx.deps.response\n",
    "    query =  ctx.deps.query\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Query: \"{query}\"\n",
    "        Response: \"{response}\"\n",
    "        \n",
    "        Does the response answer the query? Answer 'true' if yes, and 'false' if no. Don't give me or use any python code. Just answer logically \n",
    "        \"\"\"\n",
    "    \n",
    "    result = chat_model.invoke(prompt).content\n",
    "    print(result)\n",
    "    \n",
    "    # is_answering = result.strip().lower() == \"true\"\n",
    "    \n",
    "    # return JudgeResponseType(similar=is_answering)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to determine if a response answers a query, we need more information about both the query and the response.\n",
      "\n",
      "Please provide the query and the response, and I'll do my best to determine whether the response answers the query or not. \n",
      "\n",
      "For example:\n",
      "\n",
      "Query: \"What is the capital of France?\"\n",
      "Response: \"The capital of France is Paris.\"\n",
      "\n",
      "You can format your input as a dictionary with two keys: 'query' and 'response'. For example:\n",
      "```\n",
      "{\n",
      "    'query': 'What is the capital of France?',\n",
      "    'response': 'The capital of France is Paris.'\n",
      "}\n",
      "```\n",
      "\n",
      "Given this input, I will return `True` if the response answers the query in any capacity. Otherwise, I will return `False`.\n"
     ]
    }
   ],
   "source": [
    "deps = MyDeps(response=results.data, query='Tata Steel Vision')\n",
    "\n",
    "res = await judging_agent.run(\n",
    "    'Is the response is answering the query or not',\n",
    "    deps = deps\n",
    ")\n",
    "\n",
    "print(res.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ip\":\"122.161.78.200\",\"providers\":{\"dbip\":{\"country\":\"IN\",\"asn\":\"AS24560\",\"org_name\":\"Bharti Airtel Limited\",\"city\":\"Shahdara\",\"zip_code\":\"\",\"time_zone\":\"\",\"meta\":\"\\u003ca href='https://db-ip.com'\\u003eIP Geolocation by DB-IP\\u003c/a\\u003e\"},\"ip2location\":{\"country\":\"IN\",\"asn\":\"\",\"org_name\":\"\",\"city\":\"Almora\",\"zip_code\":\"263137\",\"time_zone\":\"+05:30\",\"meta\":\"This site or product includes IP2Location LITE data available from \\u003ca href=\\\"https://lite.ip2location.com\\\"\\u003ehttps://lite.ip2location.com\\u003c/a\\u003e.\"},\"ipinfo\":{\"country\":\"IN\",\"asn\":\"AS24560\",\"org_name\":\"Bharti Airtel Ltd., Telemedia Services\",\"city\":\"\",\"zip_code\":\"\",\"time_zone\":\"\",\"meta\":\"\\u003cp\\u003eIP address data powered by \\u003ca href=\\\"https://ipinfo.io\\\" \\u003eIPinfo\\u003c/a\\u003e\\u003c/p\\u003e\"},\"maxmind\":{\"country\":\"IN\",\"asn\":\"AS24560\",\"org_name\":\"Bharti Airtel Ltd., Telemedia Services\",\"city\":\"New Delhi\",\"zip_code\":\"\",\"time_zone\":\"+05:30\",\"meta\":\"This product includes GeoLite2 Data created by MaxMind, available from https://www.maxmind.com.\"}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get('https://ip.oxylabs.io/location')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research_agent = Agent(\n",
    "#     model,\n",
    "#     result_type = ResearchSchema,\n",
    "#     system_prompt = \n",
    "    \n",
    "#     '''\n",
    "#         You are a research expert agent tasked with conducting a thorough research of a company given to you. You will be given a company name using which you have to do multiple searches using the tools given to you.\n",
    "#         During these multiple searches you must use the top 5 links for each search to gather information about a company's and must gather the following information:\n",
    "\n",
    "#         1. About the Company\n",
    "#             - Industry and Market Segment:\n",
    "#                 - Primary industry (e.g., Healthcare, Automotive, Retail).\n",
    "#                 - Sub-segment specialization (e.g., diagnostic tools, electric vehicles, e-commerce logistics).\n",
    "\n",
    "#             - Key Offerings:\n",
    "#                 - Products and services.\n",
    "#                 - Unique selling propositions (USPs).\n",
    "#                 - Technological capabilities (e.g., AI-based solutions, IoT integration).\n",
    "\n",
    "#             - Strategic Focus Areas:\n",
    "#                 - Operational improvements (e.g., supply chain optimization).\n",
    "#                 - Customer experience (e.g., personalized recommendations, chatbots).\n",
    "#                 - Sustainability initiatives, if applicable.\n",
    "\n",
    "#             - Current Technology Adoption:\n",
    "#                 - Existing AI/ML systems or tools in use.\n",
    "#                 - Public partnerships with AI/ML providers (e.g., AWS, Azure, Google Cloud).\n",
    "\n",
    "#         2. About the Industry\n",
    "#             - Market Trends:\n",
    "#                 - Industry-specific AI and GenAI adoption trends.\n",
    "#                 - Emerging technologies and use cases in the sector.\n",
    "\n",
    "#             - Competitor Analysis:\n",
    "#                 - Key competitors and their offerings.\n",
    "#                 - AI/ML initiatives or breakthroughs by competitors.\n",
    "#                 - Relevant partnerships and collaborations.\n",
    "\n",
    "#             - Challenges and Opportunities:\n",
    "#                 - Pain points the industry faces.\n",
    "#                 - Opportunities for AI to address these challenges. \n",
    "\n",
    "#         You are free to do multiple searches but must provide all the data/ study done and gathered for this company based on the above points.\n",
    "#         You are a research expert tasked with gathering information about a company. Use the `searchWeb` tool to search the web and return data for the following queries:\n",
    "\n",
    "#     '''\n",
    "# )\n",
    "\n",
    "# @research_agent.tool\n",
    "# async def searchWeb(ctx: RunContext[str], query: str):\n",
    "#     \"\"\"Search across the web using query.\n",
    "\n",
    "#     Args: \n",
    "\n",
    "#         query: query to search\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Recieved Query: {query}. Starting Web Search\")\n",
    "#     list_ = DDGS().text(  \n",
    "#                 keywords = query,\n",
    "#                 region = 'wt-wt',\n",
    "#                 safesearch = 'off',\n",
    "#                 timelimit = '7d',\n",
    "#                 max_results = 5\n",
    "#             )\n",
    "\n",
    "#     headers = {\n",
    "#     \"Authorization\": f\"Bearer {os.getenv('JINA_API_KEY')}\"\n",
    "#     }\n",
    "\n",
    "#     data = \"\"\n",
    "\n",
    "#     print(f\"Web Search Complete. Starting Scraping\")\n",
    "\n",
    "#     for result in tqdm(list_, desc=\"Scraping Links\"):\n",
    "#         try:\n",
    "#             url = f\"https://r.jina.ai/{result['href']}\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             response.raise_for_status()\n",
    "#             data += response.text + \"\\n\"\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fetching URL {url}: {e}\")\n",
    "\n",
    "# results = await research_agent.run(\n",
    "#     'Tata Steel'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to the proxy:  HTTPSConnectionPool(host='ip.oxylabs.io', port=443): Max retries exceeded with url: /location (Caused by ProxyError('Unable to connect to proxy', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000226BDEF5650>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')))\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import ProxyError, ReadTimeout, ConnectTimeout\n",
    "import requests\n",
    "\n",
    "\n",
    "scheme_proxy_map = {\n",
    "    'http': \"http://2.56.215.247:3128\",\n",
    "    'https': \"https://88.198.24.108:8080\",\n",
    "    'https://example.org': \"https://my-user:aegi1Ohz@2.56.215.247:8044\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get('https://ip.oxylabs.io/location', proxies=scheme_proxy_map)\n",
    "except (ProxyError, ReadTimeout, ConnectTimeout) as error:\n",
    "        print('Unable to connect to the proxy: ', error)\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import requests\n",
    "\n",
    "q= queue.Queue()\n",
    "valid_proxies = []\n",
    "\n",
    "with open('proxies.txt', 'r', encoding='utf-8') as file:\n",
    "    proxies = file.read().split('\\n')\n",
    "    for p in proxies:\n",
    "        q.put(p)\n",
    "\n",
    "def check_proxies():\n",
    "    global q\n",
    "    while not q.empty():\n",
    "        proxy = q.get()\n",
    "        try:\n",
    "            res = requests.get(\"http://ipinfo.io/json\", proxies={\"http\":proxy, \"https\": proxy})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if res.status_code ==200:\n",
    "            print(proxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-research-agent-m8UpT5-t-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
